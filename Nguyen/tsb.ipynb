{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension.\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version:  2.11.0\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from packaging import version\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "print(\"TensorFlow version: \", tf.__version__)\n",
    "assert version.parse(tf.__version__).release[0] >= 2, \\\n",
    "    \"This notebook requires TensorFlow 2.0 or above.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.11.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorboard\n",
    "tensorboard.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense\n",
    "from keras.models import Model\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import shutil\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuNet(Model):\n",
    "    def __init__(self, neurons=128):\n",
    "        super(NeuNet, self).__init__()        \n",
    "        self.in_layer = Dense(10, activation='relu')\n",
    "        self.hidden_1 = Dense(neurons, activation='relu')\n",
    "        self.hidden_2 = Dense(neurons, activation='relu')\n",
    "        self.hidden_3 = Dense(neurons, activation='relu')\n",
    "        self.hidden_4 = Dense(neurons, activation='relu')\n",
    "        self.hidden_5 = Dense(neurons, activation='relu')\n",
    "        self.hidden_6 = Dense(neurons, activation='relu')\n",
    "        self.hidden_7 = Dense(neurons, activation='relu')\n",
    "        self.hidden_8 = Dense(neurons, activation='relu')\n",
    "        self.hidden_9 = Dense(neurons, activation='relu')\n",
    "        self.hidden_10 = Dense(neurons, activation='relu')\n",
    "        self.out_layer = Dense(1, activation='relu')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.in_layer(inputs)\n",
    "        x = self.hidden_1(x)\n",
    "        x = self.hidden_2(x)\n",
    "        x = self.hidden_3(x)\n",
    "        x = self.hidden_4(x)\n",
    "        x = self.hidden_5(x)\n",
    "        x = self.hidden_6(x)\n",
    "        x = self.hidden_7(x)\n",
    "        x = self.hidden_8(x)\n",
    "        x = self.hidden_9(x)\n",
    "        x = self.hidden_10(x)\n",
    "        outputs = self.out_layer(x)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_net(data: pd.DataFrame, labels: pd.DataFrame, net: NeuNet, loss: tf.keras.losses, optimizer: tf.keras.optimizers, train_loss, train_accuracy):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = net(data)\n",
    "        loss = loss(labels, predictions)\n",
    "    gradients = tape.gradient(loss, net.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, net.trainable_variables))\n",
    "    train_loss(loss)\n",
    "    train_accuracy(labels, predictions)\n",
    "\n",
    "\n",
    "def val_net(data: pd.DataFrame, labels: pd.DataFrame, net: NeuNet, loss: tf.keras.losses, val_loss, val_accuracy):\n",
    "    predictions = net(data)\n",
    "    loss = loss(labels, predictions)\n",
    "    val_loss(loss)\n",
    "    val_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "BUFFER_SIZE = 32\n",
    "NEURONS = 128\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dir = Path('data/prepared/')\n",
    "\n",
    "X_train_name = in_dir / 'X_full.csv'\n",
    "y_train_name = in_dir / 'y_full.csv'\n",
    "X_train = pd.read_csv(X_train_name)\n",
    "y_train = pd.read_csv(y_train_name)\n",
    "X_val_name = in_dir / 'X_val.csv'\n",
    "y_val_name = in_dir / 'y_val.csv'\n",
    "X_val = pd.read_csv(X_val_name)\n",
    "y_val = pd.read_csv(y_val_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices(\n",
    "(X_train, y_train)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\htien\\NNML\\MachineLearningCourseLabs\\.venv\\lib\\site-packages\\tensorflow\\python\\ops\\summary_ops_v2.py:1332: start (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.start` instead.\n",
      "Epoch 1/50\n",
      "230/230 [==============================] - 1s 2ms/step - loss: 18.4473 - mse: 18.4473\n",
      "Epoch 2/50\n",
      "230/230 [==============================] - 0s 2ms/step - loss: 0.3971 - mse: 0.3971\n",
      "Epoch 3/50\n",
      "230/230 [==============================] - 0s 1ms/step - loss: 0.3034 - mse: 0.3034\n",
      "Epoch 4/50\n",
      "230/230 [==============================] - 0s 1ms/step - loss: 0.2542 - mse: 0.2542\n",
      "Epoch 5/50\n",
      "230/230 [==============================] - 0s 1ms/step - loss: 0.2129 - mse: 0.2129\n",
      "Epoch 6/50\n",
      "230/230 [==============================] - 0s 1ms/step - loss: 0.1852 - mse: 0.1852\n",
      "Epoch 7/50\n",
      "230/230 [==============================] - 0s 1ms/step - loss: 0.1659 - mse: 0.1659\n",
      "Epoch 8/50\n",
      "230/230 [==============================] - 0s 1ms/step - loss: 0.1449 - mse: 0.1449\n",
      "Epoch 9/50\n",
      "230/230 [==============================] - 0s 1ms/step - loss: 0.1318 - mse: 0.1318\n",
      "Epoch 10/50\n",
      "230/230 [==============================] - 0s 1ms/step - loss: 0.1274 - mse: 0.1274\n",
      "Epoch 11/50\n",
      "230/230 [==============================] - 0s 1ms/step - loss: 0.1194 - mse: 0.1194\n",
      "Epoch 12/50\n",
      "230/230 [==============================] - 0s 2ms/step - loss: 0.1167 - mse: 0.1167\n",
      "Epoch 13/50\n",
      "230/230 [==============================] - 0s 2ms/step - loss: 0.1152 - mse: 0.1152\n",
      "Epoch 14/50\n",
      "230/230 [==============================] - 0s 2ms/step - loss: 0.1154 - mse: 0.1154\n",
      "Epoch 15/50\n",
      "230/230 [==============================] - 0s 1ms/step - loss: 0.1138 - mse: 0.1138\n",
      "Epoch 16/50\n",
      "230/230 [==============================] - 0s 1ms/step - loss: 0.1093 - mse: 0.1093\n",
      "Epoch 17/50\n",
      "230/230 [==============================] - 0s 1ms/step - loss: 0.1070 - mse: 0.1070\n",
      "Epoch 18/50\n",
      "230/230 [==============================] - 0s 1ms/step - loss: 0.1023 - mse: 0.1023\n",
      "Epoch 19/50\n",
      "230/230 [==============================] - 0s 2ms/step - loss: 0.1055 - mse: 0.1055\n",
      "Epoch 20/50\n",
      "230/230 [==============================] - 0s 1ms/step - loss: 0.1024 - mse: 0.1024\n",
      "Epoch 21/50\n",
      "230/230 [==============================] - 0s 1ms/step - loss: 0.0999 - mse: 0.0999\n",
      "Epoch 22/50\n",
      "230/230 [==============================] - 0s 2ms/step - loss: 0.1020 - mse: 0.1020\n",
      "Epoch 23/50\n",
      "230/230 [==============================] - 0s 2ms/step - loss: 0.1012 - mse: 0.1012\n",
      "Epoch 24/50\n",
      "230/230 [==============================] - 0s 2ms/step - loss: 0.0980 - mse: 0.0980\n",
      "Epoch 25/50\n",
      "230/230 [==============================] - 0s 2ms/step - loss: 0.0963 - mse: 0.0963\n",
      "Epoch 26/50\n",
      "230/230 [==============================] - 0s 1ms/step - loss: 0.0994 - mse: 0.0994\n",
      "Epoch 27/50\n",
      "230/230 [==============================] - 0s 2ms/step - loss: 0.0963 - mse: 0.0963\n",
      "Epoch 28/50\n",
      "230/230 [==============================] - 0s 2ms/step - loss: 0.0961 - mse: 0.0961\n",
      "Epoch 29/50\n",
      "230/230 [==============================] - 0s 2ms/step - loss: 0.0943 - mse: 0.0943\n",
      "Epoch 30/50\n",
      "230/230 [==============================] - 0s 2ms/step - loss: 0.0943 - mse: 0.0943\n",
      "Epoch 31/50\n",
      "230/230 [==============================] - 0s 2ms/step - loss: 0.0953 - mse: 0.0953\n",
      "Epoch 32/50\n",
      "230/230 [==============================] - 0s 2ms/step - loss: 0.0937 - mse: 0.0937\n",
      "Epoch 33/50\n",
      "230/230 [==============================] - 0s 2ms/step - loss: 0.0924 - mse: 0.0924\n",
      "Epoch 34/50\n",
      "230/230 [==============================] - 0s 2ms/step - loss: 0.0927 - mse: 0.0927\n",
      "Epoch 35/50\n",
      "230/230 [==============================] - 0s 2ms/step - loss: 0.0925 - mse: 0.0925\n",
      "Epoch 36/50\n",
      "230/230 [==============================] - 0s 2ms/step - loss: 0.0909 - mse: 0.0909\n",
      "Epoch 37/50\n",
      "230/230 [==============================] - 0s 2ms/step - loss: 0.0900 - mse: 0.0900\n",
      "Epoch 38/50\n",
      "230/230 [==============================] - 0s 2ms/step - loss: 0.0933 - mse: 0.0933\n",
      "Epoch 39/50\n",
      "230/230 [==============================] - 0s 2ms/step - loss: 0.0893 - mse: 0.0893\n",
      "Epoch 40/50\n",
      "230/230 [==============================] - 0s 2ms/step - loss: 0.0909 - mse: 0.0909\n",
      "Epoch 41/50\n",
      "230/230 [==============================] - 1s 2ms/step - loss: 0.0922 - mse: 0.0922\n",
      "Epoch 42/50\n",
      "230/230 [==============================] - 0s 2ms/step - loss: 0.0870 - mse: 0.0870\n",
      "Epoch 43/50\n",
      "230/230 [==============================] - 0s 2ms/step - loss: 0.0923 - mse: 0.0923\n",
      "Epoch 44/50\n",
      "230/230 [==============================] - 0s 2ms/step - loss: 0.0871 - mse: 0.0871\n",
      "Epoch 45/50\n",
      "230/230 [==============================] - 0s 2ms/step - loss: 0.0889 - mse: 0.0889\n",
      "Epoch 46/50\n",
      "230/230 [==============================] - 0s 2ms/step - loss: 0.0890 - mse: 0.0890\n",
      "Epoch 47/50\n",
      "230/230 [==============================] - 0s 2ms/step - loss: 0.0868 - mse: 0.0868\n",
      "Epoch 48/50\n",
      "230/230 [==============================] - 0s 2ms/step - loss: 0.0869 - mse: 0.0869\n",
      "Epoch 49/50\n",
      "230/230 [==============================] - 0s 2ms/step - loss: 0.0878 - mse: 0.0878\n",
      "Epoch 50/50\n",
      "230/230 [==============================] - 0s 2ms/step - loss: 0.0871 - mse: 0.0871\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2d49cdcae80>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logs_path = Path('logs')\n",
    "if logs_path.exists():\n",
    "    shutil.rmtree(logs_path)\n",
    "\n",
    "net = NeuNet(neurons=NEURONS)\n",
    "net.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=LEARNING_RATE), loss=tf.keras.losses.MeanSquaredError(), metrics=['mse'])\n",
    "\n",
    "\n",
    "# train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "# train_accuracy = tf.keras.metrics.MeanSquaredError(name='train_mse')\n",
    "# val_loss = tf.keras.metrics.Mean(name='val_loss')\n",
    "# val_accuracy = tf.keras.metrics.MeanSquaredError(name='val_mse')\n",
    "\n",
    "# current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "# train_log_dir = logs_path / 'gradient_tape' / current_time / 'train'\n",
    "# val_log_dir = logs_path / 'gradient_tape' / current_time / 'val'\n",
    "# train_log_dir.mkdir(exist_ok=True, parents=True)\n",
    "# val_log_dir.mkdir(exist_ok=True, parents=True)\n",
    "# train_summary_writer = tf.summary.create_file_writer(str(train_log_dir))\n",
    "# val_summary_writer = tf.summary.create_file_writer(str(val_log_dir))\n",
    "\n",
    "logdir = logs_path / \"fit\" / datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "logdir.mkdir(exist_ok=True, parents=True)\n",
    "fit_summary_writer = tf.summary.create_file_writer(str(logdir))\n",
    "\n",
    "tf.summary.trace_on(graph=True, profiler=True)\n",
    "\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "net.fit(x=X_train, y=y_train, batch_size=BATCH_SIZE, epochs=EPOCHS, callbacks=tensorboard_callback)\n",
    "                            \n",
    "# for epoch in range(EPOCHS):\n",
    "#     for (X_train, y_train) in train_ds:\n",
    "#         with fit_summary_writer.as_default():\n",
    "#             train_net(X_train, y_train, net, loss, optimizer, train_loss, train_accuracy)\n",
    "#     with train_summary_writer.as_default():\n",
    "#         tf.summary.scalar('loss', train_loss.result(), step=epoch)\n",
    "#         tf.summary.scalar('accuracy', train_accuracy.result(), step=epoch)\n",
    "#     for (X_val, y_val) in val_ds:\n",
    "#         val_net(X_val, y_val, net, loss, val_loss, val_accuracy)\n",
    "#     with val_summary_writer.as_default():\n",
    "#         tf.summary.scalar('loss', val_loss.result(), step=epoch)\n",
    "#         tf.summary.scalar('mse', val_accuracy.result(), step=epoch)\n",
    "#     template = 'Epoch {}, Loss: {}, Accuracy: {}, val Loss: {}, val MSE: {}'\n",
    "#     print (template.format(epoch+1,\n",
    "#                             train_loss.result(),\n",
    "#                             train_accuracy.result(),\n",
    "#                             val_loss.result(),\n",
    "#                             val_accuracy.result()))\n",
    "#     if epoch == EPOCHS - 1:\n",
    "#         print(\"Loss: \", val_loss.result().numpy())\n",
    "#     # Reset metrics every epoch\n",
    "#     train_loss.reset_states()\n",
    "#     val_loss.reset_states()\n",
    "#     train_accuracy.reset_states()\n",
    "#     val_accuracy.reset_states()\n",
    "# with fit_summary_writer.as_default():\n",
    "#     tf.summary.trace_export(\n",
    "#         name=\"my_func_trace\",\n",
    "#         step=0,\n",
    "#         profiler_outdir=logdir\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 17904), started 2 days, 18:10:50 ago. (Use '!kill 17904' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-89d6dea4c692a22a\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-89d6dea4c692a22a\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "18a840478b49eb6371f789d543619406b3b3692ece2f6d3ca573b3f354589b7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
